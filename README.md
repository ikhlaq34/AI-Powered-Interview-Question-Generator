# AI-Powered-Interview-Question-Generator
This tool uses the Gemma-2b LLM to generate 10 tailored technical and behavioral questions by matching intern profiles with job descriptions. Optimized for Google Colab, it employs 4-bit quantization to produce high-quality, role-specific interview scripts rapidly.
This sophisticated interview generation system leverages the Gemma-2b-it Large Language Model (LLM) to bridge the gap between candidate qualifications and organizational needs. By ingesting raw data from intern profiles and job descriptions, the script employs advanced prompt engineering to synthesize 10 targeted interview questions. The logic is bifurcated into technical assessments, which challenge the candidate's specific project history, and behavioral inquiries, which are structured around the STAR framework (Situation, Task, Action, Result) to evaluate soft skills and cultural fit.

Technically, the architecture is optimized for cloud-based deployment within the Google Colab ecosystem. It utilizes 4-bit quantization via the bitsandbytes library, which compresses the model's weights to significantly reduce VRAM consumption without compromising the quality of the generated text. By integrating the Hugging Face transformers library and accelerate module, the script handles tokenization and tensor distribution across the GPU automatically, providing a high-speed, scalable solution for HR professionals looking to modernize their recruitment workflows with generative AI.
